{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The code is built based on the supplementary files in [1].\n",
        "\n",
        "[1] Tong, X., Xu, X., Huang, S. L., & Zheng, L. (2021). A Mathematical Framework for Quantifying Transferability in Multi-source Transfer Learning. Advances in Neural Information Processing Systems, 34"
      ],
      "metadata": {
        "id": "SzJToxRKVPFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load the packages"
      ],
      "metadata": {
        "id": "U5AjJWnbVfmK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRR5uOU-a2a6"
      },
      "outputs": [],
      "source": [
        "\n",
        "import scipy.io\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plot\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd.function import Function\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from  torch.utils.data import DataLoader\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import random\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "test_cuda=True\n",
        "device = torch.device(\"cuda\" if test_cuda else \"cpu\")\n",
        "time_start=time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Load Office-Caltech-10 dataset"
      ],
      "metadata": {
        "id": "WGk1zijxVhwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "You need to download [Office-Caltech-10 DeCAF-fc6 features](https://drive.google.com/drive/folders/1XAKj5ikc6ygcanexeiWuwvnHMf5rUVGy) generated in [1] to run this code. Then: \n",
        "\n",
        "\n",
        "2.1. If you run the code on Colab, you will need to put the data in the corresponding folder of your [Google Drive](https://drive.google.com/drive/u/0/my-drive).\n",
        "\n",
        "\n",
        "2.2. If you run the code locally, you will need to put the data in the corresponding folder of your device.\n",
        "\n",
        "\n",
        "\n",
        "[1] Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., & Darrell, T. (2014, January). Decaf: A deep convolutional activation feature for generic visual recognition. In International conference on machine learning (pp. 647-655). PMLR."
      ],
      "metadata": {
        "id": "McxGQDEqvAWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount the Google drive, please ignore this cell if you run the code locally.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJcYeYH8VlL6",
        "outputId": "6aed5be9-4d0e-4d69-8a75-709392e81eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "amazon = scipy.io.loadmat('/content/drive/MyDrive/office-31/fc6/amazon_fc6.mat') # The path where you put the data.\n",
        "webcam = scipy.io.loadmat('/content/drive/MyDrive/office-31/fc6/webcam_fc6.mat') # The path where you put the data.\n",
        "dslr = scipy.io.loadmat('/content/drive/MyDrive/office-31/fc6/dslr_fc6.mat')     # The path where you put the data."
      ],
      "metadata": {
        "id": "zOswtEqEVczU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Select 10 classess data on office31 dataset"
      ],
      "metadata": {
        "id": "Dz6ukYeJWCFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "amazon_fts,amazon_labels=amazon['fts'],amazon['labels'].reshape(-1)-1\n",
        "webcam_fts,webcam_labels=webcam['fts'],webcam['labels'].reshape(-1)-1\n",
        "dslr_fts,dslr_labels=dslr['fts'],dslr['labels'].reshape(-1)-1\n",
        "print(amazon_fts.shape)\n",
        "print(amazon_labels)\n",
        "print(webcam_labels.shape)\n",
        "print(dslr_labels.shape)\n",
        "n_class=10\n",
        "\n",
        "np.random.choice\n",
        "class_set=np.array([1,2,6,11,12,13,16,17,18,23])-1\n",
        "#backpack, bike, calculator, headphones, keyboard, laptop-computer, monitor, mouse, mu, and projector\n",
        "\n",
        "def get_data(domain_name):\n",
        "  index=np.array([])\n",
        "  label_for_training=[]\n",
        "  fts,labels=None, None\n",
        "  if domain_name=='amazon':\n",
        "    fts,labels=amazon_fts,amazon_labels\n",
        "  elif domain_name=='webcam':\n",
        "    fts,labels=webcam_fts,webcam_labels\n",
        "  elif domain_name=='dslr':\n",
        "    fts,labels=dslr_fts,dslr_labels\n",
        "\n",
        "  j=0\n",
        "  for i in class_set:\n",
        "    index=np.hstack((index,np.where(labels==i)[0]))\n",
        "    for a in np.where(labels==i)[0]:\n",
        "      label_for_training.append(j)\n",
        "    j=j+1\n",
        "  index=index.astype(int)\n",
        "  return fts[index],np.array(label_for_training)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDWXYI6Nhhuz",
        "outputId": "9d8c8715-c54f-4ecf-aaf1-70d8ce325cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2817, 4096)\n",
            "[ 0  0  0 ... 30 30 30]\n",
            "(795,)\n",
            "(498,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Example on *Amaon and DSLR* domain adaptation task."
      ],
      "metadata": {
        "id": "ur_Xnp3hWq4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1,y1=get_data('amazon')\n",
        "x2,y2=get_data('dslr')"
      ],
      "metadata": {
        "id": "EaxQxQlYWetx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Mathematical Framework (MF method)"
      ],
      "metadata": {
        "id": "PHCarcbUWXW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "index=np.random.choice(x2[y2==0].shape[0],3,replace=False)\n",
        "refx=x2[y2==0][index]\n",
        "refy=[0,0,0]\n",
        "for i in range(1,10):\n",
        "    index=np.random.choice(x2[y2==i].shape[0],3,replace=False)\n",
        "    refx=np.vstack((refx,x2[y2==i][index]))\n",
        "    refy=np.append(refy,[i,i,i])\n",
        "refy=refy.astype(int)\n",
        "\n",
        "index=np.random.choice(x1[y1==0].shape[0],8,replace=False)\n",
        "sourcex=x1[y1==0][index]\n",
        "sourcey=0*np.ones(8)\n",
        "for i in range(1,10):\n",
        "    index=np.random.choice(x1[y1==i].shape[0],8,replace=False)\n",
        "    sourcex=np.vstack((sourcex,x1[y1==i][index]))\n",
        "    sourcey=np.append(sourcey,i*np.ones(8))\n",
        "sourcey=sourcey.astype(int)\n",
        "\n",
        "\n",
        "#feature---------------\n",
        "class Net_f(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net_f, self).__init__()\n",
        "        self.fc1 = nn.Linear(4096,1024)\n",
        "        self.act1= nn.PReLU()\n",
        "\n",
        "        self.fc2 = nn.Linear(1024,64)\n",
        "        self.act2= nn.PReLU()\n",
        "\n",
        "    def forward(self,x):\n",
        "        out=F.relu(self.fc1(x))\n",
        "        out=self.act1(out)\n",
        "        out=self.fc2(out)\n",
        "        out=self.act2(out)\n",
        "        return out       \n",
        "\n",
        "\n",
        "class Net_g(nn.Module):\n",
        "    def __init__(self,num_class=10, dim=64):\n",
        "        super(Net_g, self).__init__()\n",
        "\n",
        "        self.fc=nn.Linear(num_class, dim)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out=self.fc(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "def corr(f,g):\n",
        "    k = torch.mean(torch.sum(f*g,1))\n",
        "    return k\n",
        "    \n",
        "def cov_trace(f,g):\n",
        "    cov_f = torch.mm(torch.t(f),f) / (f.size()[0]-1.)\n",
        "    cov_g = torch.mm(torch.t(g),g) / (g.size()[0]-1.)\n",
        "    return torch.trace(torch.mm(cov_f, cov_g))\n",
        "\n",
        "def neg_hscore(f,g):\n",
        "    f0 = f - torch.mean(f,0)\n",
        "    g0 = g - torch.mean(g,0)\n",
        "    corr = torch.mean(torch.sum(f0*g0,1))\n",
        "    cov_f = torch.mm(torch.t(f0),f0) / (f0.size()[0]-1.)\n",
        "    cov_g = torch.mm(torch.t(g0),g0) / (g0.size()[0]-1.)\n",
        "    return - corr + torch.trace(torch.mm(cov_f, cov_g)) / 2.\n",
        "\n",
        "lr=0.0002\n",
        "epoch=100\n",
        "ind=0\n",
        "model_f = Net_f().to(device)\n",
        "model_g = Net_g().to(device)\n",
        "optimizer_fg = torch.optim.Adam(list(model_f.parameters())+list(model_g.parameters()),lr=lr)\n",
        "losslist=[]\n",
        "acclist=[0]\n",
        "alpha=[0.9,0.1]\n",
        "\n",
        "samples_ref=torch.from_numpy(refx)\n",
        "labels_ref=torch.from_numpy(refy)\n",
        "labels_one_hot_ref = torch.zeros(len(labels_ref), 10).scatter_(1, labels_ref.view(-1,1), 1)\n",
        "samples_trans=torch.from_numpy(sourcex)\n",
        "labels_trans=torch.from_numpy(sourcey)\n",
        "labels_one_hot_trans= torch.zeros(len(labels_trans), 10).scatter_(1, labels_trans.view(-1,1), 1)\n",
        "\n",
        "for i in range(epoch):\n",
        "    model_f.train()\n",
        "    model_g.train()\n",
        "    \n",
        "    f_ref=model_f(Variable(samples_ref).float().to(device))\n",
        "    g_ref=model_g(Variable(labels_one_hot_ref).float().to(device))\n",
        "    f0_ref = f_ref - torch.mean(f_ref,0)\n",
        "    g0_ref = g_ref - torch.mean(g_ref,0)\n",
        "    f_trans=model_f(Variable(samples_trans).float().to(device))-torch.mean(f_ref,0)\n",
        "    g_trans=model_g(Variable(labels_one_hot_trans).float().to(device))- torch.mean(g_ref,0)\n",
        "    optimizer_fg.zero_grad()\n",
        "    \n",
        "    loss=(-2)*alpha[0]*corr(f0_ref,g0_ref)\n",
        "    loss+=(-2)*alpha[1]*corr(f_trans,g_trans)\n",
        "    loss+=cov_trace(f0_ref,g0_ref)\n",
        "    losslist.append(loss.item())\n",
        "    loss.backward()\n",
        "    optimizer_fg.step()\n",
        "    ind+=1\n",
        "#------acc\n",
        "    model_f.eval()\n",
        "    model_g.eval()\n",
        "    fc = model_f(Variable(samples_trans).float().to(device)).data.cpu().numpy()\n",
        "    f_mean = np.sum(fc,axis=0)/fc.shape[0]\n",
        "    labellist = torch.Tensor(np.eye(10))\n",
        "    gc = model_g(Variable(labellist).to(device)).data.cpu().numpy()\n",
        "    gce = np.sum(gc,axis=0)/gc.shape[0]\n",
        "    gcp = gc-gce\n",
        "\n",
        "    samples_test=torch.from_numpy(x2)\n",
        "    labels_test = y2\n",
        "    fc=model_f(Variable(samples_test).float().to(device)).data.cpu().numpy()\n",
        "    fcp=fc-f_mean\n",
        "    fgp=np.dot(fcp,gcp.T)\n",
        "    acc = (np.argmax(fgp, axis = 1) == labels_test).sum()\n",
        "    total = len(samples_test)\n",
        "\n",
        "    samples_test=torch.from_numpy(refx)\n",
        "    labels_test = refy\n",
        "    fc=model_f(Variable(samples_test).float().to(device)).data.cpu().numpy()\n",
        "    fcp=fc-f_mean\n",
        "    fgp=np.dot(fcp,gcp.T)\n",
        "    acc1 = (np.argmax(fgp, axis = 1) == labels_test).sum()\n",
        "    total1 = len(samples_test)\n",
        "\n",
        "    acc=(acc-acc1)/(total-total1)\n",
        "#    print(acc)\n",
        "    if acc > 0.5:\n",
        "       if acc > (max(acclist)):\n",
        "           paraf=model_f.state_dict()\n",
        "           parag=model_g.state_dict()\n",
        "#           print('changepara')\n",
        "           finalacc=acc\n",
        "    acclist.append(acc)\n",
        "print(total-total1)\n",
        "print(\"Best testing accuracy among epochs:\",finalacc)\n",
        "print(\"Accuracy list for epochs:\",acclist)\n",
        "\n",
        "model_fa = Net_f().to(device)\n",
        "model_fa.load_state_dict(paraf)\n",
        "\n",
        "fstar_ref=model_f(Variable(samples_ref).float().to(device))\n",
        "ftilde_ref = fstar_ref - torch.mean(fstar_ref,0)\n",
        "py_ref=np.zeros(31)\n",
        "for i in range(31):\n",
        "    py_ref[i]=np.sum(labels_ref.numpy()==i)/labels_ref.shape[0]\n",
        "\n",
        "\n",
        "fstar_trans=model_f(Variable(samples_trans).float().to(device))\n",
        "ftilde_trans = fstar_trans - torch.mean(fstar_ref,0)\n",
        "py_trans=np.zeros(10)\n",
        "for i in range(10):\n",
        "    py_trans[i]=np.sum(labels_trans.numpy()==i)/labels_trans.shape[0]\n",
        "lambdaf= torch.mm(torch.t(ftilde_ref),ftilde_ref) / (ftilde_ref.size()[0]-1.)\n",
        "\n",
        "v_ref=0\n",
        "for i in range(10):\n",
        "    a=ftilde_ref[labels_ref==i]\n",
        "    v_ref+=(1/labels_ref.shape[0]*torch.trace(torch.mm(torch.inverse(lambdaf),(torch.mm(torch.t(a),a)/a.size()[0])))).item()\n",
        "    v_ref-=(1/labels_ref.shape[0]*py_ref[i]*(torch.mm(torch.mean(a,0).reshape(1,64),torch.inverse(lambdaf)).mm(torch.t(torch.mean(a,0).reshape(1,64))))).item()\n",
        "v_trans=0\n",
        "for i in range(10):\n",
        "    a=ftilde_trans[labels_trans==i]\n",
        "    v_trans+=(1/labels_trans.shape[0]*py_trans[i]/py_ref[i]*torch.trace(torch.mm(torch.inverse(lambdaf),(torch.mm(torch.t(a),a)/a.size()[0])))).item()\n",
        "    v_trans-=(1/labels_trans.shape[0]*py_trans[i]*py_trans[i]/py_ref[i]*(torch.mm(torch.mean(a,0).reshape(1,64),torch.inverse(lambdaf)).mm(torch.t(torch.mean(a,0).reshape(1,64))))).item()\n",
        "\n",
        "h_ref=torch.zeros((10,64))\n",
        "for i in range(10):\n",
        "    a=ftilde_ref[labels_ref==i]\n",
        "    h_ref[i]=py_ref[i]*torch.mean(a,0)\n",
        "\n",
        "h_trans=torch.zeros((10,64))\n",
        "for i in range(10):\n",
        "    a=ftilde_trans[labels_trans==i]\n",
        "    h_trans[i]=py_trans[i]*torch.mean(a,0)\n",
        "\n",
        "h=torch.zeros((64,64))\n",
        "for i in range(10):\n",
        "    h+=1/py_ref[i]*torch.mm(torch.t(h_ref[i]-h_trans[i]).reshape(64,1),(h_ref[i]-h_trans[i]).reshape(1,64))\n",
        "\n",
        "d=torch.trace(torch.mm(torch.inverse(lambdaf),h.to(device)))\n",
        "\n",
        "alpha=v_ref/(v_ref+v_trans+d.item())\n",
        "print(\"Alpha:\",alpha)\n",
        "time_end=time.time()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOXfDAgWgpVi",
        "outputId": "8ce3da73-05be-4ee7-f0d2-78c8f2f92b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "127\n",
            "Best testing accuracy among epochs: 0.8740157480314961\n",
            "Accuracy list for epochs: [0, 0.6062992125984252, 0.6929133858267716, 0.6692913385826772, 0.6062992125984252, 0.6141732283464567, 0.7007874015748031, 0.7795275590551181, 0.7795275590551181, 0.8110236220472441, 0.84251968503937, 0.8267716535433071, 0.84251968503937, 0.84251968503937, 0.8503937007874016, 0.8661417322834646, 0.8740157480314961, 0.8661417322834646, 0.8740157480314961, 0.8503937007874016, 0.8346456692913385, 0.8346456692913385, 0.8267716535433071, 0.8346456692913385, 0.8110236220472441, 0.8110236220472441, 0.7874015748031497, 0.7874015748031497, 0.7559055118110236, 0.7637795275590551, 0.7322834645669292, 0.7165354330708661, 0.7007874015748031, 0.6771653543307087, 0.6535433070866141, 0.6456692913385826, 0.6456692913385826, 0.6377952755905512, 0.6377952755905512, 0.6220472440944882, 0.6220472440944882, 0.6220472440944882, 0.6141732283464567, 0.6062992125984252, 0.5905511811023622, 0.5905511811023622, 0.5826771653543307, 0.5669291338582677, 0.5433070866141733, 0.5354330708661418, 0.5118110236220472, 0.5118110236220472, 0.5118110236220472, 0.5118110236220472, 0.5196850393700787, 0.5275590551181102, 0.5196850393700787, 0.49606299212598426, 0.48031496062992124, 0.47244094488188976, 0.4330708661417323, 0.4409448818897638, 0.4251968503937008, 0.4094488188976378, 0.41732283464566927, 0.4015748031496063, 0.4094488188976378, 0.3858267716535433, 0.3779527559055118, 0.3779527559055118, 0.3779527559055118, 0.3858267716535433, 0.4015748031496063, 0.4015748031496063, 0.4015748031496063, 0.3937007874015748, 0.3937007874015748, 0.3858267716535433, 0.4015748031496063, 0.3937007874015748, 0.3937007874015748, 0.3937007874015748, 0.3937007874015748, 0.3858267716535433, 0.3937007874015748, 0.3937007874015748, 0.3937007874015748, 0.3937007874015748, 0.3937007874015748, 0.3779527559055118, 0.3858267716535433, 0.3700787401574803, 0.3779527559055118, 0.3779527559055118, 0.3779527559055118, 0.3858267716535433, 0.3779527559055118, 0.3858267716535433, 0.3779527559055118, 0.33858267716535434, 0.3228346456692913]\n",
            "Alpha: -7.066089244856e-11\n"
          ]
        }
      ]
    }
  ]
}