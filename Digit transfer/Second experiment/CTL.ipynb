{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzxa5M-KIlaT"
      },
      "source": [
        "1. Load the packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27EecuJEIpNy"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plot\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd.function import Function\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from  torch.utils.data import DataLoader\n",
        "import torch.optim.lr_scheduler as lr_scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ImmS9EdIqix"
      },
      "source": [
        "2. Define the function used to load MNIST and USPS data splits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "You need to download[ MNIST-USPS data splits](https://github.com/samotiian/CCSA) generated in [1] to run this code. Then: \n",
        "\n",
        "\n",
        "2.1. If you run the code on Colab, you will need to put these splits in the corresponding folder of your [Google Drive](https://drive.google.com/drive/u/0/my-drive).\n",
        "\n",
        "\n",
        "2.2. If you run the code locally, you will need to put these splits in the corresponding folder of your device.\n",
        "\n",
        "\n",
        "\n",
        "[1] Motiian, S., Piccirilli, M., Adjeroh, D. A., & Doretto, G. (2017). Unified deep supervised domain adaptation and generalization. In Proceedings of the IEEE international conference on computer vision (pp. 5715-5725)."
      ],
      "metadata": {
        "id": "BfkeVwHCInpU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hBBmnnFo0Rk",
        "outputId": "0bb1d86a-3926-4ce8-e253-292e9ad01b7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount the Google drive, please ignore this cell if you run the code locally.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The path where you put the data.\n",
        "initial_path='/content/drive/MyDrive/MINIST domain adaptation/CCSA-master/row_data/'"
      ],
      "metadata": {
        "id": "9-IH9m5-I-m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ib3gPazmpSGy"
      },
      "outputs": [],
      "source": [
        "# The function used to load the data, refered to:\n",
        "##\n",
        "#Motiian, S., Piccirilli, M., Adjeroh, D. A., & Doretto, G. (2017). \n",
        "#Unified deep supervised domain adaptation and generalization. \n",
        "#In Proceedings of the IEEE international conference on computer vision (pp. 5715-5725).\n",
        "##\n",
        "\n",
        "\n",
        "def read_data(domain_adaptation_task,repetition,sample_per_class):\n",
        "    UM  = domain_adaptation_task\n",
        "    cc  = repetition\n",
        "    SpC = sample_per_class\n",
        "    if UM != 'MNIST_to_USPS':\n",
        "        if UM != 'USPS_to_MNIST':\n",
        "            raise Exception('domain_adaptation_task should be either MNIST_to_USPS or USPS_to_MNIST')\n",
        "\n",
        "\n",
        "    if cc <0 or cc>10:\n",
        "        raise Exception('number of repetition should be between 0 and 9.')\n",
        "\n",
        "    if SpC <1 or SpC>7:\n",
        "            raise Exception('number of sample_per_class should be between 1 and 7.')\n",
        "\n",
        "    X_train_target=np.load(initial_path + UM + '_X_train_target_repetition_' + str(cc) + '_sample_per_class_' + str(SpC) + '.npy')\n",
        "    y_train_target=np.load(initial_path + UM + '_y_train_target_repetition_' + str(cc) + '_sample_per_class_' + str(SpC) + '.npy')\n",
        "\n",
        "    X_train_source=np.load(initial_path + UM + '_X_train_source_repetition_' + str(cc) + '_sample_per_class_' + str(SpC) + '.npy')\n",
        "    y_train_source=np.load(initial_path + UM + '_y_train_source_repetition_' + str(cc) + '_sample_per_class_' + str(SpC) + '.npy')\n",
        "\n",
        "    X_test = np.load(initial_path + UM + '_X_test_target_repetition_' + str(cc) + '_sample_per_class_' + str(SpC)+'.npy')\n",
        "    y_test = np.load(initial_path + UM + '_y_test_target_repetition_' + str(cc) + '_sample_per_class_' + str(SpC)+'.npy')\n",
        "    \n",
        "\n",
        "    \n",
        "    return X_train_target,y_train_target,X_train_source,y_train_source,X_test,y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyBx6_XClYkG"
      },
      "source": [
        "3. Self-defined layer for implementing Center Transfer loss (CTL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbwXJgK0bToc",
        "outputId": "bb1c6652-6b47-4987-bf17-d029aeb0fcbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "[Parameter containing:\n",
            "tensor([[-0.2528,  1.4072],\n",
            "        [ 0.2910,  1.0365],\n",
            "        [-0.9816, -3.4219],\n",
            "        [ 1.4910,  0.2422],\n",
            "        [ 1.4832, -0.3704],\n",
            "        [ 0.0941,  2.1528],\n",
            "        [ 0.6271, -1.1666],\n",
            "        [-0.7862,  0.0759],\n",
            "        [-0.0086, -0.6568],\n",
            "        [-1.0011,  0.2992],\n",
            "        [ 0.6396, -1.0857],\n",
            "        [-1.6153,  1.5635],\n",
            "        [ 0.8194,  0.6117],\n",
            "        [ 0.7602,  1.4788],\n",
            "        [ 1.9647,  0.9414],\n",
            "        [ 0.3883, -0.3957],\n",
            "        [ 0.5920, -2.8563],\n",
            "        [-0.4750, -0.9978],\n",
            "        [ 0.0489,  0.9250],\n",
            "        [-1.2278, -0.9470]], device='cuda:0', requires_grad=True)]\n",
            "None\n",
            "0.9038917422294617\n"
          ]
        }
      ],
      "source": [
        "class CTL(nn.Module):\n",
        "    def __init__(self, num_classes, feat_dim, size_average=True):\n",
        "        super(CTL, self).__init__()\n",
        "        self.centers = nn.Parameter(torch.randn(num_classes, feat_dim))\n",
        "        self.centerlossfunc = CenterlossFunc.apply\n",
        "        self.feat_dim = feat_dim\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def forward(self, label, flip_label, domain_label,feat):\n",
        "        batch_size = feat.size(0)\n",
        "        feat = feat.view(batch_size, -1)\n",
        "        # To check the dim of centers and features\n",
        "        if feat.size(1) != self.feat_dim:\n",
        "            raise ValueError(\"Center's dim: {0} should be equal to input feature's \\\n",
        "                            dim: {1}\".format(self.feat_dim,feat.size(1)))\n",
        "        batch_size_tensor = feat.new_empty(1).fill_(batch_size if self.size_average else 1)\n",
        "        loss = self.centerlossfunc(feat, label,flip_label, domain_label,self.centers, batch_size_tensor)\n",
        "        return loss\n",
        "\n",
        "\n",
        "class CenterlossFunc(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, feature, label,flip_label, domain_label,centers, batch_size):\n",
        "        ctx.save_for_backward(feature, label,flip_label, domain_label,centers, batch_size)\n",
        "        centers_batch = centers.index_select(0, flip_label.long())\n",
        "\n",
        "        domain_counts = centers.new_zeros(2)\n",
        "        domain_ones = centers.new_ones(domain_label.size(0))\n",
        "        domain_counts = domain_counts.scatter_add_(0, domain_label.long(), domain_ones)+0.0001\n",
        "        domain_counts = torch.index_select(domain_counts, 0, domain_label.int())  \n",
        "\n",
        "        return ((feature - centers_batch).pow(2)/domain_counts.view(-1, 1)).sum() / 2.0 / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        feature,label,flip_label,domain_label,centers,batch_size = ctx.saved_tensors\n",
        "        flip_centers_batch = centers.index_select(0, flip_label.long())\n",
        "        flip_diff=flip_centers_batch-feature\n",
        "\n",
        "        domain_counts = centers.new_zeros(2)\n",
        "        domain_ones = centers.new_ones(domain_label.size(0))\n",
        "        domain_counts = domain_counts.scatter_add_(0, domain_label.long(), domain_ones)+0.0001\n",
        "        domain_counts = torch.index_select(domain_counts, 0, domain_label.int())  \n",
        "      \n",
        "\n",
        "\n",
        "        centers_batch = centers.index_select(0, label.long())\n",
        "        diff = centers_batch - feature\n",
        "        # init every iteration\n",
        "        counts = centers.new_ones(centers.size(0))\n",
        "        ones = centers.new_ones(label.size(0))\n",
        "        grad_centers = centers.new_zeros(centers.size())\n",
        "        counts = counts.scatter_add_(0, label.long(), ones)-1+0.001\n",
        "\n",
        "        grad_centers.scatter_add_(0, label.unsqueeze(1).expand(feature.size()).long(), diff)\n",
        "        grad_centers = grad_centers/counts.view(-1, 1)\n",
        "       \n",
        "        return - grad_output * (flip_diff/domain_counts.view(-1, 1)) / batch_size, None, None,None,grad_centers / batch_size, None\n",
        "\n",
        "#Testing function\n",
        "def main(test_cuda=False):\n",
        "    print('-'*80)\n",
        "    device = torch.device(\"cuda\" if test_cuda else \"cpu\")\n",
        "    ct = CTL(20,2,size_average=True).to(device)\n",
        "    y = torch.Tensor([0,0,2,1,3]).to(device)\n",
        "    dola= torch.Tensor([0,0,1,1,0]).to(device)\n",
        "    feat = torch.zeros(5,2).to(device).requires_grad_()\n",
        "    print(list(ct.parameters()))\n",
        "    \n",
        "    print(ct.centers.grad)\n",
        "    out = ct(y,y,dola,feat)\n",
        "    print(out.item())\n",
        "    out.backward()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    torch.manual_seed(999)\n",
        "    if torch.cuda.is_available():\n",
        "        main(test_cuda=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_MIPBlXl5gl"
      },
      "source": [
        "4. Define LetNet++ for MNIST-USPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyUwCBngl7XV"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1_1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n",
        "        self.prelu1_1 = nn.PReLU()\n",
        "        self.conv1_2 = nn.Conv2d(32, 32, kernel_size=5, padding=2)\n",
        "        self.prelu1_2 = nn.PReLU()\n",
        "        self.conv2_1 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
        "        self.prelu2_1 = nn.PReLU()\n",
        "        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
        "        self.prelu2_2 = nn.PReLU()\n",
        "        self.conv3_1 = nn.Conv2d(64, 128, kernel_size=5, padding=2)\n",
        "        self.prelu3_1 = nn.PReLU()\n",
        "        self.conv3_2 = nn.Conv2d(128, 128, kernel_size=5, padding=2)\n",
        "        self.prelu3_2 = nn.PReLU()\n",
        "        self.preluip1 = nn.PReLU()\n",
        "        \n",
        "        self.ip1 = nn.Linear(128*2*2, 84)\n",
        "        self.ip2 = nn.Linear(84, 10, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.prelu1_1(self.conv1_1(x))\n",
        "        x = self.prelu1_2(self.conv1_2(x))\n",
        "        x = F.max_pool2d(x,2)\n",
        "        x = self.prelu2_1(self.conv2_1(x))\n",
        "        x = self.prelu2_2(self.conv2_2(x))\n",
        "        x = F.max_pool2d(x,2)\n",
        "        x = self.prelu3_1(self.conv3_1(x))\n",
        "        x = self.prelu3_2(self.conv3_2(x))\n",
        "        x = F.max_pool2d(x,2)\n",
        "        x = x.view(-1, 128*2*2)\n",
        "\n",
        "        ip1 = self.preluip1(self.ip1(x))\n",
        "        ip2 = self.ip2(ip1)\n",
        "        return ip1, F.log_softmax(ip2, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGHtd3LtNjUr"
      },
      "source": [
        "5. Define the training in one epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPPfm01yNjcx"
      },
      "outputs": [],
      "source": [
        "#deine the training within 1 epoch\n",
        "\n",
        "def train(epoch,loss_weight,train_loader,model,nllloss,ctl,optimizer4nn,optimzer4center,x_test,y_test,device):\n",
        "  center_total=0\n",
        "  nll_losss=0\n",
        "  #Training in each epoch#\n",
        "\n",
        "  for data, target,good_ord,flip_ord,domain_label in train_loader:\n",
        "    data, target,good_ord,flip_ord,domain_label = data.to(device), target.to(device),good_ord.to(device), flip_ord.to(device),domain_label.to(device)\n",
        "\n",
        "    ip1, pred = model(data)\n",
        "    loss = nllloss(pred, target) + loss_weight* ctl(good_ord, flip_ord,domain_label,ip1)\n",
        "\n",
        "    optimizer4nn.zero_grad()\n",
        "    optimzer4center.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer4nn.step()\n",
        "    optimzer4center.step()\n",
        "\n",
        "    center_total=center_total+ctl(good_ord, flip_ord,domain_label,ip1).item()\n",
        "    nll_losss= nll_losss+nllloss(pred, target).item()*64\n",
        "\n",
        "  test_out = model(x_test.to(device))[1]\n",
        "  pred_y = torch.max(test_out.cpu(), 1)[1].data.numpy()\n",
        "  accuracy = float((pred_y == y_test.data.numpy()).astype(int).sum()) / float(y_test.size(0))\n",
        "  if epoch>97:\n",
        "    print(\"Training... Epoch = %d\" % epoch, 'Softmax loss:', nll_losss/len(train_loader.dataset), 'CTL:', center_total/len(train_loader))\n",
        "    print('Testing accuracy:',accuracy)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oc9WU_nNkkJ"
      },
      "source": [
        "6. Define the training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzajvfuJNkrR"
      },
      "outputs": [],
      "source": [
        "\n",
        "def trainss(ministorusps,repetition,sample_per_class,weightss,center_step):\n",
        "  #Activate GPU\n",
        "  use_cuda = torch.cuda.is_available() and True\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        " \n",
        "  repetition=repetition\n",
        "  sample_per_class=sample_per_class\n",
        "\n",
        "\n",
        "  #Load data & formulate necessary labels for implementation\n",
        "  X_train_target,y_train_target,X_train_source,y_train_source,X_test,y_test=read_data(ministorusps,repetition,sample_per_class)\n",
        "  domain_label=np.hstack((np.zeros(len(y_train_source)),np.ones(len(y_train_target))))\n",
        "  x_train=np.vstack((X_train_source,X_train_target))\n",
        "  y_train=np.hstack((y_train_source,y_train_target))\n",
        "  good_order=np.hstack((y_train_source,y_train_target+10))\n",
        "  flip_order=np.hstack((y_train_source+10,y_train_target))\n",
        "  x_train=torch.from_numpy(x_train).reshape(-1,1,16,16)\n",
        "  y_train=torch.from_numpy(y_train)\n",
        "  x_test=torch.from_numpy(X_test).reshape(-1,1,16,16)\n",
        "  y_test=torch.from_numpy(y_test)\n",
        "  print(\"Size of training set:\",len(x_train))\n",
        "  print(\"Size of testing set:\",len(x_test))\n",
        "  good_order=torch.from_numpy(good_order)\n",
        "  flip_order=torch.from_numpy(flip_order)\n",
        "  domain_label=torch.from_numpy(domain_label)\n",
        "  x_train=x_train.float()\n",
        "  x_test=x_test.float()\n",
        "  y_train=y_train.long()\n",
        "  y_test=y_test.long()\n",
        "  good_order=good_order.long()\n",
        "  flip_order=flip_order.long()\n",
        "  domain_label=domain_label.long()\n",
        "\n",
        "\n",
        "\n",
        "  # form the dataset\n",
        "  train_dataset=TensorDataset(x_train,y_train,good_order,flip_order,domain_label)\n",
        "  val_dataset=TensorDataset(x_test,y_test)\n",
        "\n",
        "\n",
        "  # form the loader\n",
        "  train_loader= DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "  test_loader = DataLoader(val_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "  # Model\n",
        "  model = Net().to(device)\n",
        "\n",
        "\n",
        "  # Softmax loss\n",
        "  nllloss = nn.CrossEntropyLoss().to(device) #CrossEntropyLoss = log_softmax + NLLLoss\n",
        "\n",
        "  # Trade-off: λ\n",
        "  loss_weight =  weightss\n",
        "\n",
        "  # CTL\n",
        "  ctl = CTL(20, 84).to(device)\n",
        "\n",
        "  # Optimizer for the network\n",
        "  optimizer4nn = optim.Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "\n",
        "  # Optimizer for the CTL layer\n",
        "  optimzer4center = torch.optim.SGD(ctl.parameters(), lr =center_step)\n",
        "\n",
        "  #100 epoches\n",
        "  for epoch in range(100):\n",
        "    train(epoch+1, loss_weight,train_loader,model,nllloss,ctl,optimizer4nn,optimzer4center,x_test,y_test,device)\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Example"
      ],
      "metadata": {
        "id": "8TN0Hd-0L_R8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below is an example of 10 repetitions of N=4, i.e., four samples per class from the target domain, on the task of USPS to MNIST.\n",
        "\n",
        "You may change the value of variables to get other experimental results.\n",
        "\n",
        "Note that models will be trained from scratch. Training time should last for around 30 seconds for one repetition if you use the GPU (GeForce RTX 3090).\n",
        "\n",
        "Although the results you get may be slightly different from the ones of the manuscript due to randomized initialization, the gap should be very small."
      ],
      "metadata": {
        "id": "053ydyabMD_C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCjuGNZ6Vsif",
        "outputId": "0b38f5c0-bd67-4360-d028-5486d0e6691c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transfer task: USPS_to_MNIST\n",
            "===========Samples per class: 4; Repeition split: 0 ============\n",
            "Size of training set: 1840\n",
            "Size of testing set: 2000\n",
            "Training... Epoch = 98 Softmax loss: 0.00022574861321355338 CTL: 0.003270946459138188\n",
            "Testing accuracy: 0.9045\n",
            "Training... Epoch = 99 Softmax loss: 0.00034267055275647535 CTL: 0.006253493355651354\n",
            "Testing accuracy: 0.902\n",
            "Training... Epoch = 100 Softmax loss: 0.0002705975635604852 CTL: 0.006300115082734104\n",
            "Testing accuracy: 0.9025\n",
            "\n",
            "\n",
            "Transfer task: USPS_to_MNIST\n",
            "===========Samples per class: 4; Repeition split: 1 ============\n",
            "Size of training set: 1840\n",
            "Size of testing set: 2000\n",
            "Training... Epoch = 98 Softmax loss: 0.0005081253708340228 CTL: 0.01297752946165615\n",
            "Testing accuracy: 0.9035\n",
            "Training... Epoch = 99 Softmax loss: 0.0007485168120499862 CTL: 0.0107786833581611\n",
            "Testing accuracy: 0.888\n",
            "Training... Epoch = 100 Softmax loss: 0.0009649726845647978 CTL: 0.014644031043581921\n",
            "Testing accuracy: 0.902\n",
            "\n",
            "\n",
            "Transfer task: USPS_to_MNIST\n",
            "===========Samples per class: 4; Repeition split: 2 ============\n",
            "Size of training set: 1840\n",
            "Size of testing set: 2000\n",
            "Training... Epoch = 98 Softmax loss: 0.0002599125424318987 CTL: 0.0012780016745793923\n",
            "Testing accuracy: 0.921\n",
            "Training... Epoch = 99 Softmax loss: 0.00024210887495428323 CTL: 0.001258468253393497\n",
            "Testing accuracy: 0.922\n",
            "Training... Epoch = 100 Softmax loss: 0.00023599883602203235 CTL: 0.0009634175663011085\n",
            "Testing accuracy: 0.9205\n",
            "\n",
            "\n",
            "Transfer task: USPS_to_MNIST\n",
            "===========Samples per class: 4; Repeition split: 3 ============\n",
            "Size of training set: 1840\n",
            "Size of testing set: 2000\n",
            "Training... Epoch = 98 Softmax loss: 0.0002839867187582928 CTL: 0.0017088204060263675\n",
            "Testing accuracy: 0.8925\n",
            "Training... Epoch = 99 Softmax loss: 0.00026780015778849305 CTL: 0.002299546432713496\n",
            "Testing accuracy: 0.8855\n",
            "Training... Epoch = 100 Softmax loss: 0.00025293138586794553 CTL: 0.001902629480825673\n",
            "Testing accuracy: 0.8885\n",
            "\n",
            "\n",
            "Transfer task: USPS_to_MNIST\n",
            "===========Samples per class: 4; Repeition split: 4 ============\n",
            "Size of training set: 1840\n",
            "Size of testing set: 2000\n",
            "Training... Epoch = 98 Softmax loss: 0.00020549193824357962 CTL: 0.0023436742826718196\n",
            "Testing accuracy: 0.9115\n",
            "Training... Epoch = 99 Softmax loss: 0.0001979336577832051 CTL: 0.0020274685230106115\n",
            "Testing accuracy: 0.911\n",
            "Training... Epoch = 100 Softmax loss: 0.0001890757956299121 CTL: 0.0019690443135412603\n",
            "Testing accuracy: 0.9105\n",
            "\n",
            "\n",
            "Transfer task: USPS_to_MNIST\n",
            "===========Samples per class: 4; Repeition split: 5 ============\n",
            "Size of training set: 1840\n",
            "Size of testing set: 2000\n",
            "Training... Epoch = 98 Softmax loss: 0.00027095429882731127 CTL: 0.005018807128714076\n",
            "Testing accuracy: 0.912\n",
            "Training... Epoch = 99 Softmax loss: 0.00022480760792108334 CTL: 0.004065347028542952\n",
            "Testing accuracy: 0.911\n",
            "Training... Epoch = 100 Softmax loss: 0.00021678752964362502 CTL: 0.003310291289255537\n",
            "Testing accuracy: 0.914\n",
            "\n",
            "\n",
            "Transfer task: USPS_to_MNIST\n",
            "===========Samples per class: 4; Repeition split: 6 ============\n",
            "Size of training set: 1840\n",
            "Size of testing set: 2000\n",
            "Training... Epoch = 98 Softmax loss: 0.0002802623299193447 CTL: 0.0016471867425495695\n",
            "Testing accuracy: 0.89\n",
            "Training... Epoch = 99 Softmax loss: 0.0003806677004119948 CTL: 0.005075543336088544\n",
            "Testing accuracy: 0.8785\n",
            "Training... Epoch = 100 Softmax loss: 0.0005103168948351042 CTL: 0.011188149628839615\n",
            "Testing accuracy: 0.901\n",
            "\n",
            "\n",
            "Transfer task: USPS_to_MNIST\n",
            "===========Samples per class: 4; Repeition split: 7 ============\n",
            "Size of training set: 1840\n",
            "Size of testing set: 2000\n",
            "Training... Epoch = 98 Softmax loss: 0.00022511481985692744 CTL: 0.001957803404617027\n",
            "Testing accuracy: 0.898\n",
            "Training... Epoch = 99 Softmax loss: 0.00021225120062413422 CTL: 0.0032557163082448572\n",
            "Testing accuracy: 0.8965\n",
            "Training... Epoch = 100 Softmax loss: 0.00024680151634485175 CTL: 0.0036986791328045316\n",
            "Testing accuracy: 0.902\n",
            "\n",
            "\n",
            "Transfer task: USPS_to_MNIST\n",
            "===========Samples per class: 4; Repeition split: 8 ============\n",
            "Size of training set: 1840\n",
            "Size of testing set: 2000\n",
            "Training... Epoch = 98 Softmax loss: 0.0002248899054551578 CTL: 0.003452857338084743\n",
            "Testing accuracy: 0.896\n",
            "Training... Epoch = 99 Softmax loss: 0.00024527400509332834 CTL: 0.006926794877629085\n",
            "Testing accuracy: 0.8915\n",
            "Training... Epoch = 100 Softmax loss: 0.00021140592061388104 CTL: 0.004790117626945521\n",
            "Testing accuracy: 0.9025\n",
            "\n",
            "\n",
            "Transfer task: USPS_to_MNIST\n",
            "===========Samples per class: 4; Repeition split: 9 ============\n",
            "Size of training set: 1840\n",
            "Size of testing set: 2000\n",
            "Training... Epoch = 98 Softmax loss: 0.00035066567918123757 CTL: 0.006112750426962458\n",
            "Testing accuracy: 0.93\n",
            "Training... Epoch = 99 Softmax loss: 0.0002494044738578732 CTL: 0.004565335621511371\n",
            "Testing accuracy: 0.926\n",
            "Training... Epoch = 100 Softmax loss: 0.00030578208945529614 CTL: 0.0055081342860799415\n",
            "Testing accuracy: 0.9335\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    Rep=i               # The number of splits, i.e., 0 to 9\n",
        "    sample_per_class=4 # 1 to 7\n",
        "    lambda_para=0.75    # λ\n",
        "    center_step=0.5     # α\n",
        "    transfer='USPS_to_MNIST'  # \"MNIST_to_USPS\" or \"USPS_to_MNIST\"\n",
        "\n",
        "\n",
        "    print(\"Transfer task:\", transfer)\n",
        "    print('===========Samples per class:',str(sample_per_class)+'; Repeition split:',str(i),'============')\n",
        "\n",
        "\n",
        "    model=trainss(transfer,Rep,sample_per_class,lambda_para,center_step) #Run the training\n",
        "    print('')\n",
        "    print('')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CTL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}