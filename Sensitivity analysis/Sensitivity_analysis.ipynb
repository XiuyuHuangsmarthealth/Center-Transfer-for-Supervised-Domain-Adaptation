{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzxa5M-KIlaT"
      },
      "source": [
        "1. Load the packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27EecuJEIpNy"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plot\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd.function import Function\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from  torch.utils.data import DataLoader\n",
        "import torch.optim.lr_scheduler as lr_scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ImmS9EdIqix"
      },
      "source": [
        "2. Define the function used to load MNIST and USPS data splits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "You need to download[ MNIST-USPS data splits](https://github.com/samotiian/CCSA) generated in [1] to run this code. Then: \n",
        "\n",
        "\n",
        "2.1. If you run the code on Colab, you will need to put these splits in the corresponding folder of your [Google Drive](https://drive.google.com/drive/u/0/my-drive).\n",
        "\n",
        "\n",
        "2.2. If you run the code locally, you will need to put these splits in the corresponding folder of your device.\n",
        "\n",
        "\n",
        "\n",
        "[1] Motiian, S., Piccirilli, M., Adjeroh, D. A., & Doretto, G. (2017). Unified deep supervised domain adaptation and generalization. In Proceedings of the IEEE international conference on computer vision (pp. 5715-5725)."
      ],
      "metadata": {
        "id": "bpVbCabO3w0h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hBBmnnFo0Rk",
        "outputId": "c5974c2c-38f5-4e8f-8719-614788a9c4e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount the Google drive, please ignore this cell if you run the code locally.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The path where you put the data.\n",
        "initial_path='/content/drive/MyDrive/MINIST domain adaptation/CCSA-master/row_data/'"
      ],
      "metadata": {
        "id": "xXa-70H130lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ib3gPazmpSGy"
      },
      "outputs": [],
      "source": [
        "# The function used to load the data, refered to:\n",
        "##\n",
        "#Motiian, S., Piccirilli, M., Adjeroh, D. A., & Doretto, G. (2017). \n",
        "#Unified deep supervised domain adaptation and generalization. \n",
        "#In Proceedings of the IEEE international conference on computer vision (pp. 5715-5725).\n",
        "##\n",
        "\n",
        "\n",
        "def read_data(domain_adaptation_task,repetition,sample_per_class):\n",
        "    UM  = domain_adaptation_task\n",
        "    cc  = repetition\n",
        "    SpC = sample_per_class\n",
        "    if UM != 'MNIST_to_USPS':\n",
        "        if UM != 'USPS_to_MNIST':\n",
        "            raise Exception('domain_adaptation_task should be either MNIST_to_USPS or USPS_to_MNIST')\n",
        "\n",
        "\n",
        "    if cc <0 or cc>10:\n",
        "        raise Exception('number of repetition should be between 0 and 9.')\n",
        "\n",
        "    if SpC <1 or SpC>7:\n",
        "            raise Exception('number of sample_per_class should be between 1 and 7.')\n",
        "\n",
        "    X_train_target=np.load(initial_path + UM + '_X_train_target_repetition_' + str(cc) + '_sample_per_class_' + str(SpC) + '.npy')\n",
        "    y_train_target=np.load(initial_path + UM + '_y_train_target_repetition_' + str(cc) + '_sample_per_class_' + str(SpC) + '.npy')\n",
        "\n",
        "    X_train_source=np.load(initial_path + UM + '_X_train_source_repetition_' + str(cc) + '_sample_per_class_' + str(SpC) + '.npy')\n",
        "    y_train_source=np.load(initial_path + UM + '_y_train_source_repetition_' + str(cc) + '_sample_per_class_' + str(SpC) + '.npy')\n",
        "\n",
        "    X_test = np.load(initial_path + UM + '_X_test_target_repetition_' + str(cc) + '_sample_per_class_' + str(SpC)+'.npy')\n",
        "    y_test = np.load(initial_path + UM + '_y_test_target_repetition_' + str(cc) + '_sample_per_class_' + str(SpC)+'.npy')\n",
        "    \n",
        "\n",
        "    \n",
        "    return X_train_target,y_train_target,X_train_source,y_train_source,X_test,y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyBx6_XClYkG"
      },
      "source": [
        "3. Self-defined layer for implementing Center Transfer loss (CTL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbwXJgK0bToc",
        "outputId": "c68beb5f-c77b-4836-8b8c-b4b597fc98df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "[Parameter containing:\n",
            "tensor([[-0.2528,  1.4072],\n",
            "        [ 0.2910,  1.0365],\n",
            "        [-0.9816, -3.4219],\n",
            "        [ 1.4910,  0.2422],\n",
            "        [ 1.4832, -0.3704],\n",
            "        [ 0.0941,  2.1528],\n",
            "        [ 0.6271, -1.1666],\n",
            "        [-0.7862,  0.0759],\n",
            "        [-0.0086, -0.6568],\n",
            "        [-1.0011,  0.2992],\n",
            "        [ 0.6396, -1.0857],\n",
            "        [-1.6153,  1.5635],\n",
            "        [ 0.8194,  0.6117],\n",
            "        [ 0.7602,  1.4788],\n",
            "        [ 1.9647,  0.9414],\n",
            "        [ 0.3883, -0.3957],\n",
            "        [ 0.5920, -2.8563],\n",
            "        [-0.4750, -0.9978],\n",
            "        [ 0.0489,  0.9250],\n",
            "        [-1.2278, -0.9470]], device='cuda:0', requires_grad=True)]\n",
            "None\n",
            "0.9038917422294617\n"
          ]
        }
      ],
      "source": [
        "class CTL(nn.Module):\n",
        "    def __init__(self, num_classes, feat_dim, size_average=True):\n",
        "        super(CTL, self).__init__()\n",
        "        self.centers = nn.Parameter(torch.randn(num_classes, feat_dim))\n",
        "        self.centerlossfunc = CenterlossFunc.apply\n",
        "        self.feat_dim = feat_dim\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def forward(self, label, flip_label, domain_label,feat):\n",
        "        batch_size = feat.size(0)\n",
        "        feat = feat.view(batch_size, -1)\n",
        "        # To check the dim of centers and features\n",
        "        if feat.size(1) != self.feat_dim:\n",
        "            raise ValueError(\"Center's dim: {0} should be equal to input feature's \\\n",
        "                            dim: {1}\".format(self.feat_dim,feat.size(1)))\n",
        "        batch_size_tensor = feat.new_empty(1).fill_(batch_size if self.size_average else 1)\n",
        "        loss = self.centerlossfunc(feat, label,flip_label, domain_label,self.centers, batch_size_tensor)\n",
        "        return loss\n",
        "\n",
        "\n",
        "class CenterlossFunc(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, feature, label,flip_label, domain_label,centers, batch_size):\n",
        "        ctx.save_for_backward(feature, label,flip_label, domain_label,centers, batch_size)\n",
        "        centers_batch = centers.index_select(0, flip_label.long())\n",
        "\n",
        "        domain_counts = centers.new_zeros(2)\n",
        "        domain_ones = centers.new_ones(domain_label.size(0))\n",
        "        domain_counts = domain_counts.scatter_add_(0, domain_label.long(), domain_ones)+0.0001\n",
        "        domain_counts = torch.index_select(domain_counts, 0, domain_label.int())  \n",
        "\n",
        "        return ((feature - centers_batch).pow(2)/domain_counts.view(-1, 1)).sum() / 2.0 / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        feature,label,flip_label,domain_label,centers,batch_size = ctx.saved_tensors\n",
        "        flip_centers_batch = centers.index_select(0, flip_label.long())\n",
        "        flip_diff=flip_centers_batch-feature\n",
        "\n",
        "        domain_counts = centers.new_zeros(2)\n",
        "        domain_ones = centers.new_ones(domain_label.size(0))\n",
        "        domain_counts = domain_counts.scatter_add_(0, domain_label.long(), domain_ones)+0.0001\n",
        "        domain_counts = torch.index_select(domain_counts, 0, domain_label.int())  \n",
        "      \n",
        "\n",
        "\n",
        "        centers_batch = centers.index_select(0, label.long())\n",
        "        diff = centers_batch - feature\n",
        "        # init every iteration\n",
        "        counts = centers.new_ones(centers.size(0))\n",
        "        ones = centers.new_ones(label.size(0))\n",
        "        grad_centers = centers.new_zeros(centers.size())\n",
        "        counts = counts.scatter_add_(0, label.long(), ones)-1+0.001\n",
        "\n",
        "        grad_centers.scatter_add_(0, label.unsqueeze(1).expand(feature.size()).long(), diff)\n",
        "        grad_centers = grad_centers/counts.view(-1, 1)\n",
        "       \n",
        "        return - grad_output * (flip_diff/domain_counts.view(-1, 1)) / batch_size, None, None,None,grad_centers / batch_size, None\n",
        "\n",
        "#Testing function\n",
        "def main(test_cuda=False):\n",
        "    print('-'*80)\n",
        "    device = torch.device(\"cuda\" if test_cuda else \"cpu\")\n",
        "    ct = CTL(20,2,size_average=True).to(device)\n",
        "    y = torch.Tensor([0,0,2,1,3]).to(device)\n",
        "    dola= torch.Tensor([0,0,1,1,0]).to(device)\n",
        "    feat = torch.zeros(5,2).to(device).requires_grad_()\n",
        "    print(list(ct.parameters()))\n",
        "    \n",
        "    print(ct.centers.grad)\n",
        "    out = ct(y,y,dola,feat)\n",
        "    print(out.item())\n",
        "    out.backward()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    torch.manual_seed(999)\n",
        "    if torch.cuda.is_available():\n",
        "        main(test_cuda=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_MIPBlXl5gl"
      },
      "source": [
        "4. Define LetNet++ for MNIST-USPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyUwCBngl7XV"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1_1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n",
        "        self.prelu1_1 = nn.PReLU()\n",
        "        self.conv1_2 = nn.Conv2d(32, 32, kernel_size=5, padding=2)\n",
        "        self.prelu1_2 = nn.PReLU()\n",
        "        self.conv2_1 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
        "        self.prelu2_1 = nn.PReLU()\n",
        "        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
        "        self.prelu2_2 = nn.PReLU()\n",
        "        self.conv3_1 = nn.Conv2d(64, 128, kernel_size=5, padding=2)\n",
        "        self.prelu3_1 = nn.PReLU()\n",
        "        self.conv3_2 = nn.Conv2d(128, 128, kernel_size=5, padding=2)\n",
        "        self.prelu3_2 = nn.PReLU()\n",
        "        self.preluip1 = nn.PReLU()\n",
        "        \n",
        "        self.ip1 = nn.Linear(128*2*2, 84)\n",
        "        self.ip2 = nn.Linear(84, 10, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.prelu1_1(self.conv1_1(x))\n",
        "        x = self.prelu1_2(self.conv1_2(x))\n",
        "        x = F.max_pool2d(x,2)\n",
        "        x = self.prelu2_1(self.conv2_1(x))\n",
        "        x = self.prelu2_2(self.conv2_2(x))\n",
        "        x = F.max_pool2d(x,2)\n",
        "        x = self.prelu3_1(self.conv3_1(x))\n",
        "        x = self.prelu3_2(self.conv3_2(x))\n",
        "        x = F.max_pool2d(x,2)\n",
        "        x = x.view(-1, 128*2*2)\n",
        "\n",
        "        ip1 = self.preluip1(self.ip1(x))\n",
        "        ip2 = self.ip2(ip1)\n",
        "        return ip1, F.log_softmax(ip2, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGHtd3LtNjUr"
      },
      "source": [
        "5. Define the training in one epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPPfm01yNjcx"
      },
      "outputs": [],
      "source": [
        "#deine the training within 1 epoch\n",
        "\n",
        "def train(epoch,loss_weight,train_loader,model,nllloss,centerloss,optimizer4nn,optimzer4center,x_test,y_test,device):\n",
        "  center_total=0\n",
        "  nll_losss=0\n",
        "  #Training in each epoch#\n",
        "  for data, target,good_ord,flip_ord,domain_label in train_loader:\n",
        "    data, target,good_ord,flip_ord,domain_label = data.to(device), target.to(device),good_ord.to(device), flip_ord.to(device),domain_label.to(device)\n",
        "\n",
        "    ip1, pred = model(data)\n",
        "    loss = nllloss(pred, target) + loss_weight* centerloss(good_ord, flip_ord,domain_label,ip1)\n",
        "\n",
        "    optimizer4nn.zero_grad()\n",
        "    optimzer4center.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer4nn.step()\n",
        "    optimzer4center.step()\n",
        "\n",
        "    center_total=center_total+centerloss(good_ord, flip_ord,domain_label,ip1).item()\n",
        "    nll_losss= nll_losss+nllloss(pred, target).item()*64\n",
        "  test_out = model(x_test.to(device))[1]\n",
        "  pred_y = torch.max(test_out.cpu(), 1)[1].data.numpy()\n",
        "  accuracy = float((pred_y == y_test.data.numpy()).astype(int).sum()) / float(y_test.size(0))\n",
        "\n",
        "  return accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oc9WU_nNkkJ"
      },
      "source": [
        "6. Define the training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzajvfuJNkrR"
      },
      "outputs": [],
      "source": [
        "#Activate GPU\n",
        "def trainss(ministorusps,repetition,sample_per_class,weightss,center_step):\n",
        "\n",
        "  use_cuda = torch.cuda.is_available() and True\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        " \n",
        "  repetition=repetition\n",
        "  sample_per_class=sample_per_class\n",
        "  # MNIST_to_USPS\n",
        "  X_train_target,y_train_target,X_train_source,y_train_source,X_test,y_test=read_data(ministorusps,repetition,sample_per_class)\n",
        "  \n",
        "\n",
        "  domain_label=np.hstack((np.zeros(len(y_train_source)),np.ones(len(y_train_target))))\n",
        "\n",
        "  x_train=np.vstack((X_train_source,X_train_target))\n",
        "  y_train=np.hstack((y_train_source,y_train_target))\n",
        "  \n",
        "  good_order=np.hstack((y_train_source,y_train_target+10))\n",
        "  flip_order=np.hstack((y_train_source+10,y_train_target))\n",
        "\n",
        "\n",
        "  x_train=torch.from_numpy(x_train).reshape(-1,1,16,16)\n",
        "  y_train=torch.from_numpy(y_train)\n",
        "  x_test=torch.from_numpy(X_test).reshape(-1,1,16,16)\n",
        "  y_test=torch.from_numpy(y_test)\n",
        "\n",
        "  good_order=torch.from_numpy(good_order)\n",
        "  flip_order=torch.from_numpy(flip_order)\n",
        "  domain_label=torch.from_numpy(domain_label)\n",
        "\n",
        "\n",
        "\n",
        "  x_train=x_train.float()\n",
        "  x_test=x_test.float()\n",
        "  y_train=y_train.long()\n",
        "  y_test=y_test.long()\n",
        "\n",
        "  good_order=good_order.long()\n",
        "  flip_order=flip_order.long()\n",
        "  domain_label=domain_label.long()\n",
        "  # form the dataset\n",
        "  train_dataset=TensorDataset(x_train,y_train,good_order,flip_order,domain_label)\n",
        "  val_dataset=TensorDataset(x_test,y_test)\n",
        "\n",
        "\n",
        "\n",
        "  train_loader= DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "  test_loader = DataLoader(val_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "  # Model\n",
        "  model = Net().to(device)\n",
        "\n",
        "\n",
        "  # Softmax\n",
        "  nllloss = nn.CrossEntropyLoss().to(device) \n",
        "\n",
        "  # λ\n",
        "  loss_weight =  weightss\n",
        "\n",
        "  # CTL\n",
        "  ctl = CTL(20, 84).to(device)\n",
        "\n",
        "  # optimzer4nn\n",
        "  optimizer4nn = optim.Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "  sheduler = lr_scheduler.StepLR(optimizer4nn,20,gamma=0.8)\n",
        "\n",
        "  # optimzer4center\n",
        "  optimzer4center = torch.optim.SGD(ctl.parameters(), lr =center_step) #α\n",
        "  best=[0]\n",
        "  eps=0\n",
        "  for epoch in range(100):\n",
        "    accuracy=train(epoch+1, loss_weight,train_loader,model,nllloss,ctl,optimizer4nn,optimzer4center,x_test,y_test,device)\n",
        "    if accuracy > max(best):\n",
        "      best.append(accuracy)\n",
        "      eps=epoch\n",
        "\n",
        "  print(\"Testing accuracy:\", str(max(best)))\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Sensitivity analysis on λ"
      ],
      "metadata": {
        "id": "0E1L7Ch90XQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the result is slightly different from that displayed in the manuscript. The difference is due to the random initilization. You can still see the **stable performance** of the models. "
      ],
      "metadata": {
        "id": "GUKexwl5LxgO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCjuGNZ6Vsif",
        "outputId": "3c0fc307-c378-4eac-89c8-2791045f7df7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================== λ=0 ====================\n",
            "Testing accuracy: 0.826\n",
            "==================== λ=0.1 ====================\n",
            "Testing accuracy: 0.9095\n",
            "==================== λ=0.25 ====================\n",
            "Testing accuracy: 0.907\n",
            "==================== λ=0.5 ====================\n",
            "Testing accuracy: 0.9045\n",
            "==================== λ=0.75 ====================\n",
            "Testing accuracy: 0.919\n",
            "==================== λ=1 ====================\n",
            "Testing accuracy: 0.904\n",
            "==================== λ=2.5 ====================\n",
            "Testing accuracy: 0.914\n",
            "==================== λ=5 ====================\n",
            "Testing accuracy: 0.9185\n",
            "==================== λ=7.5 ====================\n",
            "Testing accuracy: 0.919\n",
            "==================== λ=10 ====================\n",
            "Testing accuracy: 0.9095\n",
            "==================== λ=100 ====================\n",
            "Testing accuracy: 0.914\n"
          ]
        }
      ],
      "source": [
        "Rep=1\n",
        "sample_per_class=3\n",
        "center_step=0.5\n",
        "lambda_para_set=[0,0.1,0.25,0.5,0.75,1,2.5,5,7.5,10,100]\n",
        "for i in lambda_para_set:\n",
        "  print('='*20,'λ='+str(i),'='*20)\n",
        "  model=trainss('USPS_to_MNIST',Rep,sample_per_class,i,center_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Sensitivity analysis on α"
      ],
      "metadata": {
        "id": "h3EbE0tY5Orx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Rep=1\n",
        "sample_per_class=3\n",
        "center_step=[0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
        "lambda_para=0.75\n",
        "for i in center_step:\n",
        "  print('='*20,'α='+str(i),'='*20)\n",
        "  model=trainss('USPS_to_MNIST',Rep,sample_per_class,lambda_para,i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YE9yYanO0nn4",
        "outputId": "5fdf3058-0249-4bc6-fc6f-7a1c766a4f65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================== α=0.01 ====================\n",
            "Testing accuracy: 0.8345\n",
            "==================== α=0.05 ====================\n",
            "Testing accuracy: 0.872\n",
            "==================== α=0.1 ====================\n",
            "Testing accuracy: 0.92\n",
            "==================== α=0.2 ====================\n",
            "Testing accuracy: 0.9185\n",
            "==================== α=0.3 ====================\n",
            "Testing accuracy: 0.903\n",
            "==================== α=0.4 ====================\n",
            "Testing accuracy: 0.911\n",
            "==================== α=0.5 ====================\n",
            "Testing accuracy: 0.921\n",
            "==================== α=0.6 ====================\n",
            "Testing accuracy: 0.897\n",
            "==================== α=0.7 ====================\n",
            "Testing accuracy: 0.8945\n",
            "==================== α=0.8 ====================\n",
            "Testing accuracy: 0.921\n",
            "==================== α=0.9 ====================\n",
            "Testing accuracy: 0.9125\n",
            "==================== α=1 ====================\n",
            "Testing accuracy: 0.9\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Sensitivity_analysis.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}